{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device mps with dtype torch.float16\n",
      "Loaded detection model vikp/surya_layout3 on device mps with dtype torch.float16\n",
      "Loaded reading order model vikp/surya_order on device mps with dtype torch.float16\n",
      "Loaded recognition model vikp/surya_rec2 on device mps with dtype torch.float16\n",
      "Loaded texify model to mps with torch.float16 dtype\n",
      "Loaded recognition model vikp/surya_tablerec on device mps with dtype torch.float16\n"
     ]
    }
   ],
   "source": [
    "from models import load_all_models\n",
    "model_lst = load_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n",
      "Recognizing Text: 100%|██████████| 37/37 [00:09<00:00,  3.79it/s]\n",
      "Detecting bboxes: 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]\n",
      "Finding reading order: 100%|██████████| 7/7 [00:02<00:00,  3.06it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "Recognizing Text:   0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.51 GiB. GPU 0 has a total capacity of 23.55 GiB of which 1.46 GiB is free. Process 10966 has 1.04 GiB memory in use. Process 52004 has 5.96 GiB memory in use. Including non-PyTorch memory, this process has 15.02 GiB memory in use. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m fpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jenny/Downloads/Archive/wilders_grove_yard_waste/WI0500447/WI0500447_Correspondence_20190529.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# fpath = \"/home/jenny/Downloads/Archive 2/wilders_grove_yard_waste/WI0500447/WI0500447_Staff Report_20200911.pdf\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# fpath = \"/home/jenny/Downloads/01005_Eakes Drycleaners_BF Site Assmt.pdf\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m full_text, doc_images, out_meta, pages, text_blocks \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_single_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_lst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/marker/convert.py:127\u001b[0m, in \u001b[0;36mconvert_single_pdf\u001b[0;34m(fname, model_lst, max_pages, start_page, metadata, langs, batch_multiplier, ocr_all_pages)\u001b[0m\n\u001b[1;32m    124\u001b[0m indent_blocks(pages)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Fix table blocks\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m table_count \u001b[38;5;241m=\u001b[39m \u001b[43mformat_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_rec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m out_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m table_count\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/marker/tables/table.py:90\u001b[0m, in \u001b[0;36mformat_tables\u001b[0;34m(pages, doc, fname, detection_model, table_rec_model, ocr_model)\u001b[0m\n\u001b[1;32m     87\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mdisable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# This will redo OCR if OCR is forced, since we need to redetect bounding boxes, etc.\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m table_rec \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_ocr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m cells \u001b[38;5;241m=\u001b[39m [assign_rows_columns(tr, im_size) \u001b[38;5;28;01mfor\u001b[39;00m tr, im_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(table_rec, img_sizes)]\n\u001b[1;32m     92\u001b[0m table_md \u001b[38;5;241m=\u001b[39m [formatter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, cell)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m cells]\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/tabled/inference/recognition.py:47\u001b[0m, in \u001b[0;36mrecognize_tables\u001b[0;34m(table_imgs, table_cells, needs_ocr, models)\u001b[0m\n\u001b[1;32m     44\u001b[0m ocr_cells \u001b[38;5;241m=\u001b[39m [[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cells] \u001b[38;5;28;01mfor\u001b[39;00m cells, needs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(table_cells, needs_ocr) \u001b[38;5;28;01mif\u001b[39;00m needs]\n\u001b[1;32m     45\u001b[0m ocr_langs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(ocr_images)\n\u001b[0;32m---> 47\u001b[0m ocr_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_langs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_cells\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Assign text to correct spot\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orig_idx, ocr_pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(needs_ocr_idx, ocr_predictions):\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/ocr.py:31\u001b[0m, in \u001b[0;36mrun_recognition\u001b[0;34m(images, langs, rec_model, rec_processor, bboxes, polygons, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     all_slices\u001b[38;5;241m.\u001b[39mextend(slices)\n\u001b[1;32m     29\u001b[0m     all_langs\u001b[38;5;241m.\u001b[39mextend([deepcopy(lang)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(slices))\n\u001b[0;32m---> 31\u001b[0m rec_predictions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_langs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m predictions_by_image \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m slice_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/recognition.py:94\u001b[0m, in \u001b[0;36mbatch_recognition\u001b[0;34m(images, languages, model, processor, batch_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, batch_pixel_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], encoder_batch_size):\n\u001b[1;32m     93\u001b[0m     encoder_pixel_values \u001b[38;5;241m=\u001b[39m batch_pixel_values[z:\u001b[38;5;28mmin\u001b[39m(z \u001b[38;5;241m+\u001b[39m encoder_batch_size, batch_pixel_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m---> 94\u001b[0m     encoder_hidden_states_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_pixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         encoder_hidden_states \u001b[38;5;241m=\u001b[39m encoder_hidden_states_batch\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/model/recognition/encoder.py:838\u001b[0m, in \u001b[0;36mDonutSwinModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    832\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[1;32m    834\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    835\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 838\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m last_hidden_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings[:, :last_hidden_state\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :]\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/model/recognition/encoder.py:710\u001b[0m, in \u001b[0;36mDonutSwinEncoder.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    702\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m         always_partition,\n\u001b[1;32m    708\u001b[0m     )\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    715\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/model/recognition/encoder.py:628\u001b[0m, in \u001b[0;36mDonutSwinStage.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    626\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    634\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/model/recognition/encoder.py:581\u001b[0m, in \u001b[0;36mDonutSwinLayer.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    578\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(attention_windows)\n\u001b[1;32m    580\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m--> 581\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(layer_output)\n\u001b[1;32m    584\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m (layer_output, attention_outputs[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m (layer_output,)\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/surya/model/recognition/encoder.py:445\u001b[0m, in \u001b[0;36mDonutSwinIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    444\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 445\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/source_codes/marker/.venv/lib/python3.10/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.51 GiB. GPU 0 has a total capacity of 23.55 GiB of which 1.46 GiB is free. Process 10966 has 1.04 GiB memory in use. Process 52004 has 5.96 GiB memory in use. Including non-PyTorch memory, this process has 15.02 GiB memory in use. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from convert import convert_single_pdf\n",
    "fpath=\"/Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Permit (Issuance)_20230524.pdf\"\n",
    "full_text, doc_images, out_meta, pages, text_blocks = convert_single_pdf(fpath, model_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1|-7',\n",
       "  [184.0, 94.04461319411486, 728.0, 679.3222591362127],\n",
       "  'Division of Waste Management December 23, 1997 Mr. Phil Vorsatz\\n\\n![0_image_0.png](0_image_0.png)\\n\\n\\n\\n![0_image_1.png](0_image_1.png)\\n\\n\\n\\n![0_image_2.png](0_image_2.png)\\n\\n\\nNC CERCLA Project Officer EPA Region IV Waste Division 345 Courtland Street, NE\\nAtlanta, Georgia 30365 RE:\\nBrownfield Site Assessment Report Eakes Dry Cleaners.\\n\\nDurham, Durham County, North Carolina Dear Mr. Vorsatz:\\nThis letter is being sent to you to convey the information that the North Carolina Superfund Section has obtained on a site named Eakes Dry Cleaners.\\n\\nEakes Cleaners is located at 827 West Morgan Street in Durham, North Carolina.  It consists of a 6,600 square foot brick and block building situated on approximately 0.3 acres. The site was a Dry Cleaning and Fur Storage business for many years until its closure in 1995, after the death of Mr. Eakes and the bankruptcy of his estate. The site is located in a mixed commercial, light industrial, and residential area in downtown Durham.  The site is located at North 36º 00\\' 02.5\" and West 078º 54\\' 36.0\".',\n",
       "  'Text',\n",
       "  False],\n",
       " ['1|-9',\n",
       "  [182.0, 697.3308020882772, 710.0, 873.4143331751305],\n",
       "  'On October 14, 1996 Front Royal Environmental Services, Inc. submitted a Limited Soil and Groundwater Investigation Report to Mr. John A. Northen, Trustee in Bankruptcy for the site. This report revealed significant soil and groundwater contamination. The primary contaminants are tetrachloroethylene and its degradation products; trichloroethylene, trans-1,2dichloroethene, 1,1dichloroethene, and vinyl chloride. These contaminants are associated with the dry cleaning process and are concentrated in two areas; outside the Boiler/Distillation room and outside the old sump area along the south wall of the building. The remaining contaminants, benzene, toluene, ethylbenzene, xylenes (BTEX), and naphthalene are associated with an underground heating oil tank located at the northwest corner of the building.',\n",
       "  'Text',\n",
       "  False],\n",
       " ['1|-10',\n",
       "  [230.0, 992.4708115804461, 721.0, 1024.4859990507832],\n",
       "  '\\n\\n![0_image_3.png](0_image_3.png)\\n\\n',\n",
       "  'Page-footer',\n",
       "  True],\n",
       " ['2|-1',\n",
       "  [423.0, 89.04224015187471, 474.0, 106.05030849549122],\n",
       "  '\\nTable 1\\n',\n",
       "  'Caption',\n",
       "  False],\n",
       " ['2|-2',\n",
       "  [414.0, 221.89468690702088, 1374.0, 1137.4601518026566],\n",
       "  '\\n|           |                                                                                                                                                                             | Soil Analytical Results (mg/kg or ppm)   |                             |\\n|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|-----------------------------|\\n| Sample ID | Date                                                                                                                                                                        | Depth(feet)                              | Analytical Results          |\\n|           | Collected                                                                                                                                                                   |                                          |                             |\\n| ਟੰਡ\\\\-I     | 9/11/96                                                                                                                                                                     | 3.5                                      | 534 diesel                  |\\n| SS\\\\-2     | ਰੇ/11/96                                                                                                                                                                     | 3.0                                      | 81.3 diesel                 |\\n| द्रद\\\\-उ    | ਰੇ/11/96                                                                                                                                                                     | 2.5                                      | BDL diesel 0.0124 trans DCA |\\n|           |                                                                                                                                                                             |                                          | 0.0338 vinyl chloride       |\\n| द्रदे\\\\-४    | ਰੋ\\\\I I / ) / ) / ) / ) / ) / ) / ( / ) / ) / ) / ) / ( / ) / ) / ) / ) / ) / ) / ) / ) / ) / ( / ) / ) / ) / ) / ) / ) / ) / ) / ) / ) / ) / / ) / ) / / ) / / ) / / / / ) / | 3.0                                      | 0.0375 trans DCA            |\\n|           |                                                                                                                                                                             |                                          | 0.005 toluene               |\\n|           |                                                                                                                                                                             |                                          | 0.0705 vinyl chloride       |\\n| ટર્ટ\\\\-ર    | ਰੇ/I I /ਰੇਵ                                                                                                                                                                   | 3.0                                      | 0.00611 toluene             |\\n|           |                                                                                                                                                                             |                                          | 0.00506 PCE                 |\\n| દ્રક\\\\-୧    | ਰੇ/11/96                                                                                                                                                                     | 3.0                                      | BDL TPH                     |\\n|           |                                                                                                                                                                             |                                          | BDL 8240                    |\\n| SS\\\\-7     | 9/11/96                                                                                                                                                                     | 3.0                                      | 0.174 PCE                   |\\n| ટર્ટ\\\\-8    | 9/27/96                                                                                                                                                                     | 7.0                                      | 0.005 TCE                   |\\n| ટર્ટ\\\\-ે     | 9/27/96                                                                                                                                                                     | 9.0                                      | 0.152 TCE                   |\\n|           |                                                                                                                                                                             |                                          | 0.0383 PCE                  |\\n',\n",
       "  'Table',\n",
       "  False],\n",
       " ['2|-11',\n",
       "  [276.0, 573.2719506407215, 578.0, 619.2937826293308],\n",
       "  'trans DCA\\nPCE\\nll\\n=\\ntrans-1,2 dichloroethene perchlorethylene trichloroethylene JCE\\n=',\n",
       "  'Text',\n",
       "  True],\n",
       " ['3|-11',\n",
       "  [51.0, 14.0, 928.0, 163.0],\n",
       "  '45CADIT\\n15\\n-11: 2 - 1\\n ..\"\\n0 BEST\\nFARTHING\\nOHOBXO\\n(11-JA\\n\\nBVA\\n\\n![2_image_0.png](2_image_0.png)\\n\\n\\n\\n![2_image_2.png](2_image_2.png)\\n\\n',\n",
       "  'Text',\n",
       "  False],\n",
       " ['3|-12',\n",
       "  [460.0, 164.0, 495.0, 180.0],\n",
       "  '\\n\\n![2_image_1.png](2_image_1.png)\\n\\n',\n",
       "  'Figure',\n",
       "  False],\n",
       " ['3|-21',\n",
       "  [310.0, 218.0, 959.0, 322.0],\n",
       "  'ીતા ત 18)\\n17 195 111255\\n\\n![2_image_3.png](2_image_3.png)\\n\\n\\n\\n![2_image_7.png](2_image_7.png)\\n\\n\\n篇 TE\\nANDER',\n",
       "  'Text',\n",
       "  False],\n",
       " ['3|-23',\n",
       "  [243.0, 410.0, 329.0, 463.0],\n",
       "  '\\n\\n![2_image_18.png](2_image_18.png)\\n\\n\\n\\n\\n![2_image_6.png](2_image_6.png)\\n\\n\\n\\n![2_image_8.png](2_image_8.png)\\n\\n',\n",
       "  'Figure',\n",
       "  False],\n",
       " ['3|-49',\n",
       "  [112.0, 334.0, 1029.0, 516.0],\n",
       "  'と・Watt\\n\\n![2_image_5.png](2_image_5.png)\\n\\n\\nGREGSON\\nبا .....\\n\\n![2_image_4.png](2_image_4.png)\\n\\n\\nـــ\\nO G\\n15834818 0\\n\\n![2_image_9.png](2_image_9.png)\\n\\n\\n\\n![2_image_12.png](2_image_12.png)\\n\\n\\n10)\\n\\n![2_image_11.png](2_image_11.png)\\n\\n\\n\\n![2_image_14.png](2_image_14.png)\\n\\n\\n\\n![2_image_23.png](2_image_23.png)\\n\\n\\n11\\n\\n![2_image_10.png](2_image_10.png)\\n\\n\\n\\n![2_image_13.png](2_image_13.png)\\n\\n\\n\\n![2_image_15.png](2_image_15.png)\\n\\n\\n\\n![2_image_19.png](2_image_19.png)\\n\\n\\n\\n![2_image_21.png](2_image_21.png)\\n\\n\\n\\n![2_image_22.png](2_image_22.png)\\n\\n\\n15\\n\\n![2_image_24.png](2_image_24.png)\\n\\n\\nત ર\\n\\n![2_image_16.png](2_image_16.png)\\n\\n\\n\\n![2_image_17.png](2_image_17.png)\\n\\n\\n\\n![2_image_20.png](2_image_20.png)\\n\\n\\n125 RESERVE\\n师 Mi el sils\\n\\n![2_image_25.png](2_image_25.png)\\n\\n\\n\\n![2_image_26.png](2_image_26.png)\\n\\n\\nD\\n( US 7 - 7)\\nCAROLINA\\n2017 อัลการ ALABA\\nT\\nប្រទេក្រ 1-11\\nે નિર 2',\n",
       "  'Text',\n",
       "  True],\n",
       " ['4|-1',\n",
       "  [263.0, 296.0, 402.0, 385.0],\n",
       "  '\\n\\n![3_image_0.png](3_image_0.png)\\n\\n',\n",
       "  'Figure',\n",
       "  False],\n",
       " ['4|-20',\n",
       "  [787.0, 778.0, 1592.0, 227.0],\n",
       "  'company of the company\\n:\\n ବି  ୧୯୮୮ ମସିହାର FIGURE\\n4 SCALE\\n1\" = 15\\' PROJECT \\\\# 2239-96-183 ROYA\\nENAIRONMENTAL SEBAICES, IN\\nE\\nRONT\\nWEL\\n/MONITOR\\n MA\\nEAKES DRYCLEANERS\\nDURHAM, NORTH CAROLINA\\nBORING,\\nCATION \\n11',\n",
       "  'Text',\n",
       "  True],\n",
       " ['5|-1',\n",
       "  [442.0, 230.0, 1368.0, 940.0],\n",
       "  '\\n|                   |         | Groundwater Analytical Results on 9/27/96 (ppb)   |        |       |\\n|-------------------|---------|---------------------------------------------------|--------|-------|\\n| Analyte           | NCAC 2L | MW\\\\-1                                             | MW\\\\-2  | MW\\\\-3 |\\n| PCE               | 0.7     | 13.2                                              | 29,200 | ર્સ્ટ   |\\n| TCE               | 2.8     | 7.85                                              | 5,830  | ୧୫୪   |\\n| l.ldichloroethene | 0.0     | BDL                                               | 33.8   | 10.5  |\\n| trans DCA         | 70.0    | BDL                                               | ୧୦ I   | 71.2  |\\n| Vinyl Chloride    | 0.015   | BDF                                               | 4,000  | ୧। ર  |\\n| Benzene           | 1.0     | BDL                                               | 49.3   | BDL   |\\n| Ethylbenzene      | 29.0    | BDL                                               | 38.5   | BDL   |\\n| Toluene           | 1000.0  | BDL                                               | રવંત્તે   | BDL   |\\n| Xylenes           | 530.0   | BDL                                               | 68.3   | BDL   |\\n| Naphthalene       | 21.0    | પટ                                                | 33.8   | ਮੰਟ    |\\n',\n",
       "  'Table',\n",
       "  False],\n",
       " ['5|-12',\n",
       "  [182.0, 476.0, 710.0, 827.0],\n",
       "  'trans DCA\\n=\\n ==\\nPCE\\nTCE\\n ==\\ntrans-1.2 dichloroethene perchlorethylene trichloroethylene .\\n\\nThe monitoring wells installed by Front Royal were all screened in sandy clay at depths ranging from 7 to 12 feet.  the static groundwater levels were from 4 to 6 feet below land surface.\\n\\nEven though the groundwater has been heavily contaminated, the groundwater pathway is not a pathway of concern. The North Carolina Division of Public Health, Public Water Supply Section lists the closest community well (Tyndrum Subdivision) approximately 5 miles from the site.  No individual potable wells have been observed and none are suspected in the vicinity of the site since the entire area is served by the Durham City Water Department.\\n\\nSurface water drainage from the site is collected by the storm sewers and conveyed for approximately 0.5 miles. The drainage is then carried in an intermittent stream for another 0.5 miles before the stream becomes perennial. The unnamed perennial stream empties into Ellerbe Creek after another 0.5 miles.\\n\\nThe pathway runs 8 miles through Ellerby Creek and ends in Falls Lake.\\n\\nSince the area around the site is commercial/light industry, attribution of contaminants, if any were found, to the site would be very difficult.',\n",
       "  'Text',\n",
       "  False],\n",
       " ['6|-1',\n",
       "  [183.0, 94.0, 704.0, 427.0],\n",
       "  'The groundwater in the area of the site is very shallow, 4 to 6 feet deep.\\n\\nThe concentrations of the volatile organic compounds (VOC\\'s) are very high in monitoring well \\\\#2 (MW-2). The combination of these situations led us to examine  the possibility that contaminants were migrating into basements and buildings around the site from the off-gassing of the contaminants from groundwater. To address this possibility, a survey of construction of the surrounding buildings was performed by the North Carolina Superfund Section on December 6, 1996.  The Durham Magnet Center, part of the Durham Magnet Schools Program, at 400 N. Duke Street was the only building in the vicinity of the site that had a basement area. All of the other surrounding building were constructed in the \"slab on grade\" method.\\n\\nAir sampling was performed by David Lilley, Industrial Hygienist for the Superfund Section on March 6, 1997 at seven locations including a background and the Magnet School. These  samples were collected using a modified NIOSH\\n1007 Method and analyzed for tetrachloroethylene, trichloroethylene, and vinyl chloride. The samples were collected by low flow personal sampling pumps, through 150 mg. charcoal tubes equipped with a back-up section for a target duration of 8 hours, Then the contaminants were desorbed from the charcoal using carbon disulfide and analyzed on a gas chromatograph with a flame ionization detector.',\n",
       "  'Text',\n",
       "  False],\n",
       " ['6|-4',\n",
       "  [179.0, 447.0, 707.0, 732.0],\n",
       "  'None of the samples collected indicated the presents of any of the contaminants tested for, including BETX. The sample numbers followed by a B\\nare the analysis of the back-up section of the sample tubes.\\n\\nDue to the inability to locate any receptors using or threatened by the contaminated groundwater, the groundwater pathway is not a pathway of concern.\\n\\nNo release to the surface water pathway has been documented and no significant receptors are threatened; therefore, the surface water pathway is not a pathway of concern. Contaminant levels found in the soils are very low, with the possible exception of the petroleum products associated with the heating oil tank, and no on-site targets exist, consequently, the soil exposure pathway is not considered a pathway of concern. The outdoor air is not an area of concern and sampling has demonstrated that the indoor air in surrounding buildings has not been contaminated.\\n\\nBecause of the above factors, the North Carolina Superfund Section recommends that this site receives a status of No Further Remedial Action Planned under CERCLA/SARA (NFRAP).',\n",
       "  'Text',\n",
       "  False],\n",
       " ['6|-6',\n",
       "  [227.0, 751.0, 636.0, 764.0],\n",
       "  'If you have any questions, please contact me at (919) 733-2801.',\n",
       "  'Text',\n",
       "  False],\n",
       " ['6|-7',\n",
       "  [511.0, 784.0, 579.0, 796.0],\n",
       "  'Sincerely.\\n\\n![5_image_0.png](5_image_0.png)\\n\\n',\n",
       "  'Figure',\n",
       "  False],\n",
       " ['6|-11',\n",
       "  [178.0, 832.0, 672.0, 891.0],\n",
       "  'Harry Zinn Environmental Engineer Special Projects  Branch NC Superfund Section Enclosures HZ',\n",
       "  'Text',\n",
       "  True]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[block.id, block.bbox, block.text, block.block_type, block.page_end] for i, block in enumerate(text_blocks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "allfn = os.listdir(\"/home/jenny/Downloads/Archive 2/wilders_grove_yard_waste/WI0500447\")\n",
    "pdfs = set([fn for fn in allfn if fn.endswith(\".pdf\")])\n",
    "mds = set([fn for fn in allfn if not fn.endswith(\".pdf\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from convert import convert_single_pdf\n",
    "from marker.schema.page import Page\n",
    "from output import save_markdown\n",
    "import pandas as pd\n",
    "\n",
    "def page_to_dict(page: Page) -> dict:\n",
    "    # Manually convert Page to dictionary if necessary\n",
    "    return {\n",
    "        'bbox':page.bbox,\n",
    "        'pnum': page.pnum,\n",
    "        'rotation': page.rotation,\n",
    "        'text_lines': page.text_lines,  # Assuming text_lines can be converted\n",
    "        'layout': page.layout,          # Assuming layout can be converted\n",
    "        'order': page.order,\n",
    "        'ocr_method': page.ocr_method,\n",
    "        'char_blocks': page.char_blocks,  # Assuming lists of dicts can be converted\n",
    "        'images': page.images,           # Handle images if needed\n",
    "    }\n",
    "def get_dir_pdfs(parent_dir):\n",
    "    if not os.path.exists(parent_dir):\n",
    "        raise Exception(\"Starting directory '%s' does not exist\" % parent_dir)\n",
    "    return [fn for fn in os.listdir(parent_dir) if \".pdf\" in fn]\n",
    "def to_refdocid(s):\n",
    "    return (\n",
    "        s.replace(\".pdf\",\"\").replace(\" \", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\".\", \"-\")\n",
    "        .replace(\"#\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "    )\n",
    "def rename_pdf(parent_dir, filename, new_filename):\n",
    "    old_file = os.path.join(parent_dir, filename)\n",
    "    new_file = os.path.join(parent_dir, new_filename)\n",
    "    os.rename(old_file, new_file)\n",
    "    \n",
    "def run(parent_dir, output_fn, override_existing=False):\n",
    "    pdf_fns = get_dir_pdfs(parent_dir)\n",
    "    all_tblocks = []\n",
    "    all_pages = []\n",
    "    error_files = []\n",
    "    for fn in pdf_fns:\n",
    "        docid = to_refdocid(fn)\n",
    "        rename_pdf(parent_dir, fn, docid + \".pdf\")\n",
    "        input_path = os.path.join(parent_dir, docid + \".pdf\")\n",
    "        if override_existing is False and os.path.exists(input_path.replace(\".pdf\", \"\")):\n",
    "            continue\n",
    "        try:\n",
    "            full_text, images, out_meta, pages, tblocks = run_single(input_path, docid)\n",
    "            subfolder_path = save_markdown(parent_dir, fn, full_text, images, out_meta)\n",
    "            pd.DataFrame(tblocks, columns=['id', 'bbox', 'text', 'type', 'page_end']).to_csv(f\"{subfolder_path}/{docid}_blocks.csv\")\n",
    "            page_data = [page_to_dict(page) for page in pages]\n",
    "            pd.DataFrame(page_data).to_csv(f\"{subfolder_path}/{docid}_pages.csv\")\n",
    "            all_pages.append([docid, page_data[0]['bbox']])\n",
    "            pd.DataFrame(all_pages).to_csv(f\"{output_fn}_pages.csv\")\n",
    "            print(f\"Processed {len(tblocks)} blocks for {docid}\")\n",
    "            all_tblocks.extend(tblocks)\n",
    "            pd.DataFrame(all_tblocks, columns=['id', 'bbox', 'text', 'type', 'page_end']).to_csv(f\"{output_fn}_blocks.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_path}: {e}\")\n",
    "            error_files.append(docid)\n",
    "    return all_tblocks, all_pages, error_files\n",
    "\n",
    "def run_single(input_path, docid):\n",
    "    full_text, images, out_meta, pages, text_blocks = convert_single_pdf(input_path, model_lst)\n",
    "    tblocks = [[f\"{docid}-|{block.id}\", block.bbox, block.text, block.block_type, block.page_end] for i, block in enumerate(text_blocks)]\n",
    "    return full_text, images, out_meta, pages, tblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 8/8 [00:16<00:00,  2.07s/it]\n",
      "Recognizing Text:   0%|          | 0/3 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Permit_Issuance_19940831.pdf: MPS backend out of memory (MPS allocated: 8.66 GB, other allocations: 9.45 GB, max allowed: 18.13 GB). Tried to allocate 26.25 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Renewal_Application_20160729.pdf: MPS backend out of memory (MPS allocated: 8.66 GB, other allocations: 9.45 GB, max allowed: 18.13 GB). Tried to allocate 32.96 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Email_re_Outfall_3_20210401.pdf: MPS backend out of memory (MPS allocated: 8.66 GB, other allocations: 9.45 GB, max allowed: 18.13 GB). Tried to allocate 32.96 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Email_re_Invoice_20200107.pdf: MPS backend out of memory (MPS allocated: 8.66 GB, other allocations: 9.45 GB, max allowed: 18.13 GB). Tried to allocate 43.95 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_DMR_Upload_Review_20220205.pdf: MPS backend out of memory (MPS allocated: 8.66 GB, other allocations: 9.45 GB, max allowed: 18.13 GB). Tried to allocate 32.96 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting bboxes: 100%|██████████| 125/125 [02:50<00:00,  1.37s/it]\n",
      "Recognizing Text:   0%|          | 0/60 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_HISTORICAL_WITH_APPLICATION_20111011.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 13.12 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Inspection_Report_20221207.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 24.72 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Inspection_Report_20051003.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 32.96 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_DMR_SW.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 16.48 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting bboxes: 100%|██████████| 10/10 [00:17<00:00,  1.70s/it]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:16<00:00, 16.81s/it]\n",
      "Detecting bboxes: 100%|██████████| 7/7 [00:48<00:00,  7.00s/it]\n",
      "Finding reading order:  14%|█▍        | 1/7 [00:45<04:33, 45.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Permit_Issuance_20230524.pdf: MPS backend out of memory (MPS allocated: 9.68 GB, other allocations: 8.37 GB, max allowed: 18.13 GB). Tried to allocate 96.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 45/45 [01:04<00:00,  1.44s/it]\n",
      "Recognizing Text:   0%|          | 0/9 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_MONITORING_INFO_20181028.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 13.12 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_DMR_SW_3.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 16.48 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Renewal_Application_Update_20190311.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 24.72 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting bboxes:   0%|          | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_COMPLIANCE_20110823.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 32.96 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Email_re_Electronic_Signature_Agreement_20201211.pdf: MPS backend out of memory (MPS allocated: 9.66 GB, other allocations: 8.47 GB, max allowed: 18.13 GB). Tried to allocate 8.24 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 2/2 [00:08<00:00,  4.49s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:24<00:00, 24.75s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:04<00:00,  4.36s/it]\n",
      "Recognizing tables: 100%|██████████| 2/2 [00:15<00:00,  7.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 12 blocks for NCS000050_Permit_Contact_Update_Request_20220916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n",
      "Recognizing tables: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 blocks for NCS000050_eDMR_Update_Letter_20201104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:03<00:00,  3.26s/it]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:13<00:00, 13.64s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:08<00:00,  8.92s/it]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:08<00:00,  8.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 blocks for NCS000050_Inspection_Report_20090917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 3/3 [00:09<00:00,  3.17s/it]\n",
      "Detecting bboxes: 100%|██████████| 2/2 [00:14<00:00,  7.06s/it]\n",
      "Finding reading order: 100%|██████████| 2/2 [00:54<00:00, 27.42s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Recognizing tables: 100%|██████████| 3/3 [00:25<00:00,  8.56s/it]\n",
      "Recognizing equations: 100%|██████████| 7/7 [06:55<00:00, 59.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 72 blocks for NCS000050_MONITORING_INFO_20191118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:16<00:00, 16.26s/it]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:29<00:00, 29.46s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:09<00:00,  9.22s/it]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:08<00:00,  8.22s/it]\n",
      "Recognizing tables: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 blocks for NCS000050_Amendment_New_Process_20190311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n",
      "Detecting bboxes: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]\n",
      "Finding reading order: 100%|██████████| 2/2 [00:26<00:00, 13.23s/it]\n",
      "Recognizing tables: 100%|██████████| 3/3 [00:24<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22 blocks for NCS000050_DMR_SW_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 24/24 [00:40<00:00,  1.67s/it]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:15<00:00, 15.42s/it]\n",
      "Detecting bboxes: 100%|██████████| 16/16 [01:00<00:00,  3.76s/it]\n",
      "Finding reading order:   6%|▋         | 1/16 [00:31<07:55, 31.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Fact_sheet_binder_20230524.pdf: MPS backend out of memory (MPS allocated: 9.70 GB, other allocations: 8.37 GB, max allowed: 18.13 GB). Tried to allocate 96.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Permit_Contact_Update_Request_20230712.pdf: MPS backend out of memory (MPS allocated: 9.70 GB, other allocations: 8.37 GB, max allowed: 18.13 GB). Tried to allocate 87.89 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_MONITORING_INFO_20190403.pdf: MPS backend out of memory (MPS allocated: 9.70 GB, other allocations: 8.37 GB, max allowed: 18.13 GB). Tried to allocate 87.89 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_2021_DMR_20211222.pdf: MPS backend out of memory (MPS allocated: 9.70 GB, other allocations: 8.36 GB, max allowed: 18.13 GB). Tried to allocate 175.78 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting bboxes: 100%|██████████| 8/8 [00:19<00:00,  2.46s/it]\n",
      "Detecting bboxes: 100%|██████████| 5/5 [00:27<00:00,  5.56s/it]\n",
      "Finding reading order:  40%|████      | 2/5 [01:00<01:30, 30.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/jennyxu/Downloads/Archive 2/scm_metals/NCS000050_Permit_Issuance_19991119.pdf: MPS backend out of memory (MPS allocated: 9.70 GB, other allocations: 8.37 GB, max allowed: 18.13 GB). Tried to allocate 96.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 blocks for NCS000050_Email_RE_Renewal_Application_Amendments_20190314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:07<00:00,  7.95s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:03<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7 blocks for NCS000050_Email_RE_Cooling_Tower_Release_20211108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Finding reading order: 100%|██████████| 1/1 [00:16<00:00, 16.02s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Recognizing Text:   0%|          | 0/1 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_blocks, all_pages, errors \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/jennyxu/Downloads/Archive 2/scm_metals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNCS000050\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(parent_dir, output_fn, override_existing)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     full_text, images, out_meta, pages, tblocks \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     subfolder_path \u001b[38;5;241m=\u001b[39m save_markdown(parent_dir, fn, full_text, images, out_meta)\n\u001b[1;32m     53\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(tblocks, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_end\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_blocks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m, in \u001b[0;36mrun_single\u001b[0;34m(input_path, docid)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(input_path, docid):\n\u001b[0;32m---> 67\u001b[0m     full_text, images, out_meta, pages, text_blocks \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_single_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_lst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     tblocks \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, block\u001b[38;5;241m.\u001b[39mbbox, block\u001b[38;5;241m.\u001b[39mtext, block\u001b[38;5;241m.\u001b[39mblock_type, block\u001b[38;5;241m.\u001b[39mpage_end] \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_blocks)]\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_text, images, out_meta, pages, tblocks\n",
      "File \u001b[0;32m~/Desktop/projects/marker/marker/convert.py:129\u001b[0m, in \u001b[0;36mconvert_single_pdf\u001b[0;34m(fname, model_lst, max_pages, start_page, metadata, langs, batch_multiplier, ocr_all_pages)\u001b[0m\n\u001b[1;32m    126\u001b[0m indent_blocks(pages)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Fix table blocks\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m table_count \u001b[38;5;241m=\u001b[39m \u001b[43mformat_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_rec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m out_meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m table_count\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n",
      "File \u001b[0;32m~/Desktop/projects/marker/marker/tables/table.py:106\u001b[0m, in \u001b[0;36mformat_tables\u001b[0;34m(pages, doc, fname, detection_model, table_rec_model, ocr_model)\u001b[0m\n\u001b[1;32m    103\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mdisable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# This will redo OCR if OCR is forced, since we need to redetect bounding boxes, etc.\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m table_rec \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_ocr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_rec_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_ocr_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m cells \u001b[38;5;241m=\u001b[39m [assign_rows_columns(tr, im_size) \u001b[38;5;28;01mfor\u001b[39;00m tr, im_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(table_rec, img_sizes)]\n\u001b[1;32m    108\u001b[0m table_md \u001b[38;5;241m=\u001b[39m [formatter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, cell)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m cells]\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/tabled/inference/recognition.py:49\u001b[0m, in \u001b[0;36mrecognize_tables\u001b[0;34m(table_imgs, table_cells, needs_ocr, models, table_rec_batch_size, ocr_batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m ocr_cells \u001b[38;5;241m=\u001b[39m [[c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cells] \u001b[38;5;28;01mfor\u001b[39;00m cells, needs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(table_cells, needs_ocr) \u001b[38;5;28;01mif\u001b[39;00m needs]\n\u001b[1;32m     47\u001b[0m ocr_langs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(ocr_images)\n\u001b[0;32m---> 49\u001b[0m ocr_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_langs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_cells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mocr_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Assign text to correct spot\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orig_idx, ocr_pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(needs_ocr_idx, ocr_predictions):\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/ocr.py:31\u001b[0m, in \u001b[0;36mrun_recognition\u001b[0;34m(images, langs, rec_model, rec_processor, bboxes, polygons, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     all_slices\u001b[38;5;241m.\u001b[39mextend(slices)\n\u001b[1;32m     29\u001b[0m     all_langs\u001b[38;5;241m.\u001b[39mextend([deepcopy(lang)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(slices))\n\u001b[0;32m---> 31\u001b[0m rec_predictions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_langs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m predictions_by_image \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m slice_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/recognition.py:124\u001b[0m, in \u001b[0;36mbatch_recognition\u001b[0;34m(images, languages, model, processor, batch_size)\u001b[0m\n\u001b[1;32m    122\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m token_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#TODO: add attention mask\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_decoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_text_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_prefill\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m decoder_position_ids \u001b[38;5;241m=\u001b[39m decoder_position_ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    133\u001b[0m logits \u001b[38;5;241m=\u001b[39m return_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m][:current_batch_size] \u001b[38;5;66;03m# Ignore batch padding\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/model/recognition/decoder.py:618\u001b[0m, in \u001b[0;36mSuryaOCRDecoder.forward\u001b[0;34m(self, input_ids, cache_position, attention_mask, encoder_hidden_states, encoder_attention_mask, use_cache, prefill, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    609\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mLongTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    617\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, OCRModelOutput]:\n\u001b[0;32m--> 618\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    631\u001b[0m     all_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/model/recognition/decoder.py:518\u001b[0m, in \u001b[0;36mSuryaOCRDecoderModel.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache, output_hidden_states, return_dict, prefill)\u001b[0m\n\u001b[1;32m    514\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    515\u001b[0m             residual_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, position_ids, causal_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache\n\u001b[1;32m    516\u001b[0m         )\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresidual_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(hidden_states)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# add hidden states from the last decoder layer\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/model/recognition/decoder.py:382\u001b[0m, in \u001b[0;36mSuryaOCRDecoderLayer.forward\u001b[0;34m(self, activations, position_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, cache_position, use_cache)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     inputs_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_pre_norm(cross_attn_output)  \u001b[38;5;66;03m# RMSNorm introduces slight slight differences\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_attn\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m raw_activations\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/marker/.venv/lib/python3.11/site-packages/surya/model/recognition/decoder.py:263\u001b[0m, in \u001b[0;36mSuryaOCRDecoderSdpaAttention.forward\u001b[0;34m(self, hidden_states, position_ids, attention_mask, cache_position, use_cache, window_attn)\u001b[0m\n\u001b[1;32m    260\u001b[0m         position_mask[:, :, :, :current_cache_position \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(position_mask, torch\u001b[38;5;241m.\u001b[39mfinfo(causal_mask\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin, causal_mask)\n\u001b[0;32m--> 263\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    273\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_blocks, all_pages, errors = run(\"/Users/jennyxu/Downloads/Archive 2/scm_metals\", \"NCS000050\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
